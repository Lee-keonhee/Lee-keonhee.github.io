---
layout: post
title:  "ReLU, ELU, GELU 활성화함수 비교"
date:   2025-07-28 14:00:00 +0900
categories: [딥러닝, 인공지능, 데이터과학]
tags: [딥러닝 기초]
---

------


ReLU(Rectified Linear Unit)

ReLU
특징: 입력값이 0보다 작으면 0을 출력하고, 0보다 크면 입력값을 그대로 출력합니다 (max(0, x)).
쓰임새: 심층 신경망(DNN)에서 주로 사용되며, 가장 널리 사용되는 활성화 함수 중 하나입니다. 
장점: 기울기 소실 문제를 완화하고, 계산이 단순하여 학습 속도가 빠르다는 장점이 있습니다.

ReLU의 문제점
1. Dying ReLU
- 입력값이 0 이하이면 항상 0을 출력


오차 역전파
오차) 역전파는 딥러닝에서의 기울기 계산 알고리즘
과정: 순전파 - loss 구함 - 그 다음 출력층 -> 입력층 방향으로,  '각 가중치가 loss에 얼마나 영향을 미쳤는지' 계산. (수학적) 손실함수를 각 층의 가중치에 대해 미분하는 방식으로 그래디언트 계산. *게산량이 줄었다는 내용이 포인트 *체인룰 설명 있으면 좋을듯
단점: 역전파는 기울기소실 빠질 위험 =>  ReLU 함수 이용, Batch 정규화 방식

가중치 초기화
신경망은 가중치를 기준으로 데이터를 학습하고 학습 초기에는 가중치들이 정해지지 않기 때문에 가중치 초기화를 진행해야합니다. 
weight를 모두 0으로 초기화를 할시 가중합 결과는 항상 0이 되고 활성화 함수는 늘 같은 값만 출력하게 되서 학습이 제대로 되지 않습니다.
그래서 여러 초기화 방법을 사용하는데 가중치를 작은 난수로 초기화 하는 무작위초기화, 가중치를 층의 입력 및 출력 뉴런수에 따라 조정하는 자비에 초기화, 각 노드의 출력 분산이 입력분산과 동일하고록 가중치를 초기화 하는 He초기화가 있습니다. 
자비에 초기화와 He초기화의 차이는 활성화 함수에 차이가 있습니다. 자비에는 시그모이드와 하이퍼볼릭 탄젠트에 많이 사용하며, He초기화는 Relu 활성화 함수에 사용됩니다. 
또 입력 노드 수만 고려하여 자비에보다 분산을 더 크게 잡습니다. 
입력이 0보다 작으면 출력도 0이 되는데 평균적으로 절반의 뉴런만 활성화가 되기 때문에 분산을 더 크게 설정하는 것입니다

활성화 함수


옵티마이저 - 기계 학습에서 모델의 학습 과정을 조절하는 알고리즘이다.
손실 함수의 기울기를 활용해 최소값을 찾기 위해 모델의 매개변수를 조정하는 역할을 한다.
옵티마이저는 학습 과정을 가속화하고, 학습 시간을 단축 시킬 수 있음.
전역 최소값에 더 가까워 질 수 있음
모델에 따라 옵티마이저를 직접 찾아야 할 수 있음.
과적합 방지를 위해 추가적인 조치(조기 종료, 드롭 아웃)가 필요할 수 있음.


드랍아웃 
- 일정비율의 한 층의 노드를 무작위로 0으로 만듦으로써, 학습할시에 해당 노드의 정보가 학습에 반영되지 않도록 한다. 그렇게해서 데이터의 과적합(overfitting)을 줄이게 되는 기법이 Drop-out입니다.