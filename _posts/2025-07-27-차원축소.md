---
layout: post
title:  "차원 축소"
date:   2025-07-27 09:00:00 +0900
categories: [머신러닝, 인공지능, 데이터과학]
tags: [머신러닝기초]
---
---
## 차원 축소
---
&nbsp;데이터가 가지고 있는 수많은 특성(Feature)들 중에서 데이터의 본질적인 정보는 최대한 보존하면서, 특성의 개수(차원)를 줄여 더 낮은 차원의 데이터로 변환하는 과정.
&nbsp;해당 과정을 통해 데이터의 특성이 많아짐에 따라 발생할 수 있는 문제를 해결할 수 있음. 
<br>
### 차원 축소가 필요한 이유
1. ***차원의 저주***
 &nbsp;차원이 증가하면서 데이터 공간이 급격히 커지게 됨에 따라 실제로 존재하는 데이터포인터들이 희소(Sparse)하게 분포되는 현상. 즉, 데이터포인터들 간의 거리가 멀어지고 데이터 공간이 비어있는 것처럼 느껴지는 현상
2. 모델의 성능 향상
&nbsp;차원을 줄임에 따른 노이즈 제거 및 과적합 방지를 통해 모델의 일반화 성능이 향상
3. 학습 시간 및 비용 절감
&nbsp;데이터의 차원을 줄이게 되면, 학습할 파라미터의 수가 감소하여 시간 및 비용이 절감
4. 데이터 시각화의 용이성

---

## PCA (Principal Component Analysis) - 주성분 분석
---
### **PCA(Principal Component Analysis)**
&nbsp;고차원 데이터를 다루는 데 유용한 차원 축소(Dimensionality Reduction) 기법. 
&nbsp;PCA는 데이터의 본질적인 정보 손실을 최소화하면서, 더 적은 수의 새로운 '요약된 특성' (주성분)으로 데이터를 변환하여 분석과 학습을 도움.
<br>

### PCA의 핵심 목표
&nbsp;PCA의 기본 목표는 **데이터의 분산을 가장 잘 설명하는 새로운 축(주성분)**을 찾는 것.
&nbsp;즉, 데이터를 가장 넓게 펼쳐 보여줄 수 있는 방향을 찾아내어, 데이터의 중요한 특징을 잃지 않고 변환하는 것을 의미.
<br>

### 작동 원리

1. 데이터 분산 파악: 각 특성(변수)들이 얼마나 퍼져 있고, 특성들 간에 어떤 관계(공분산)가 있는지 파악.
2. 새로운 축(주성분) 생성: 원래 특성들을 선형 조합하여, 데이터의 분산을 가장 잘 나타내는 새로운 직교(orthogonal) 축들을 생성.
3. 데이터 투영: 원본 데이터를 이 새로운 축들(주성분) 위에 투영하여, 저차원의 새로운 데이터셋을 생성.
<br>

### 공분산 (Covariance) - 관계 파악의 핵심
&nbsp;PCA는 데이터의 분산과 특성들 간의 관계를 파악하기 위해 **공분산(Covariance)**을 사용.

- 정의:
&nbsp;두 변수가 함께 어떻게 변하는지를 나타내는 값으로, 한 변수가 증가할 때 다른 변수가 증가하는 경향이 있는지, 감소하는 경향이 있는지, 아니면 아무 관계가 없는지 등을 확인할 수 있음.

- 공분산 값의 의미:
&nbsp;양의 공분산: 두 변수가 같은 방향으로 움직이는 경향이 큽니다 (하나가 증가할 때 다른 하나도 증가).
&nbsp;음의 공분산: 두 변수가 반대 방향으로 움직이는 경향이 큽니다 (하나가 증가할 때 다른 하나는 감소).
&nbsp;0에 가까운 공분산: 두 변수 간에 특별한 선형 관계가 없음을 나타냅니다.

- 상관계수(Correlation)와의 차이:
**상관계수(Pearson 상관계수)**는 공분산 값을 각 변수의 표준편차로 나누어 정규화한 값입니다. 이 때문에 값의 범위가 항상 -1부터 1 사이로 정해져 변수 스케일에 상관없이 관계의 강도를 비교할 수 있습니다.
공분산은 원본 스케일에 따라 값이 달라지기 때문에, 두 변수 간의 선형 관계 방향은 알 수 있지만 강도를 비교하기 어렵습니다.

- 공분산 수식 (두 변수 X, Y에 대한 표본 공분산): 

### PCA의 작동 방식
PCA는 다음의 체계적인 과정을 통해 데이터의 차원을 축소합니다.

1. 데이터 표준화 (Standardization)
&nbsp;각 특성의 평균을 0으로, 표준편차를 1로 만들어 모든 특성이 동일한 스케일을 가지도록 합니다. 이는 스케일이 큰 특성이 분산에 불균형적으로 영향을 미 미치는 것을 방지하기 위함입니다.

2. 공분산 행렬(Covariance Matrix) 계산
&nbsp;표준화된 데이터셋의 공분산 행렬을 계산합니다. 이 행렬은 모든 특성 쌍 간의 공분산을 포함하며, 데이터의 전체적인 분산 구조를 나타냅니다.
수식:

3. 고유값(Eigenvalue) 및 고유벡터(Eigenvector) 계산
&nbsp;계산된 공분산 행렬의 고유값과 고유벡터를 추출합니다.
&nbsp;고유벡터: 데이터의 분산 방향을 나타내는 새로운 축(주성분)을 의미합니다.
&nbsp;고유값: 해당 고유벡터(축) 방향으로 데이터가 얼마나 많은 분산을 가지는지를 나타냅니다. 고유값이 클수록 해당 주성분이 데이터의 더 많은 정보를 설명합니다.

4. 주성분 선택:
고유값이 가장 큰 k개의 고유벡터를 선택합니다. 이 고유벡터들이 우리가 보존하고자 하는 핵심 주성분이 됩니다.
이 과정은 데이터의 분산을 최대한 보존하면서 차원을 줄이는 것을 목표로 합니다.
PCA는 D1을 최대화(즉, D2를 최소화)하는 고유벡터를 선택하여 데이터의 핵심 정보를 보존
- 데이터의 분산(D3): 원점에서 데이터 포인트까지의 거리 
- 보존되는 분산(D1): 원점에서 새로운 주성분에 투영된 점까지의 거리
- 손실되는 분산(D2): 데이터 포인트에서 새로운 주성원에 투영된 점까지의 거리
![주성분 분석](/assets/images/pca.png)
<br>
5. 데이터 변환(투영):
&nbsp;선택된 k개의 주성분(고유벡터)을 이용하여 원본 표준화된 데이터를 이 새로운 저차원 공간으로 투영(선형 변환)합니다. 그 결과, k개의 특성으로 이루어진 새로운 차원 축소된 데이터셋이 생성됩니다.