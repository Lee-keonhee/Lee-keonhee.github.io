---
layout: post
title:  "딥러닝 용어 정리"
date:   2025-07-29 14:00:00 +0900
categories: [딥러닝, 인공지능, 데이터과학]
tags: [딥러닝 기초]
---

------

## 퍼셉트론

---

**퍼셉트론(Perceptron)**은 인공신경망의 가장 기본적인 형태로, 인간 뇌의 뉴런을 단순화하여 모델링한 것입니다.

주요 요소:
- 노드(Node): 정보를 처리하는 기본 단위로, 입력값을 받아 가중치(W)를 곱합니다.
- 연결(Connection): 노드들이 연결되어 있으며, 이 연결을 통해 데이터가 입력부터 출력까지 흐릅니다.
- 동작 원리: 입력값에 가중치를 곱하여 모두 더한 값(가중합)이 임계값을 넘으면 1을, 넘지 못하면 0을 출력하는 단순한 이진 분류를 수행합니다.
<br>
단층 퍼셉트론: 입력층과 출력층으로만 이루어진 구조로, 주로 이진 분류나 단순 회귀 문제에 활용됩니다. 하지만 비선형적이고 복잡한 데이터 학습에는 한계가 있습니다.

---

## 다층 퍼셉트론 (MLP)

---

**다층 퍼셉트론(Multi-Layer Perceptron, MLP)**은 단층 퍼셉트론의 한계를 극복하기 위해 등장했습니다. 여러 개의 퍼셉트론이 계층(Layer) 형태로 연결된 구조를 가집니다.

구조:
입력층(Input Layer)
은닉층(Hidden Layer): 하나 이상 존재할 수 있습니다.
출력층(Output Layer)
주요 특징: 단일 퍼셉트론이 해결할 수 없었던 XOR 문제와 같은 비선형적인 문제들을 해결할 수 있습니다.

---

## 은닉층과 순전파/역전파

---

1. 은닉층 (Hidden Layer)
위치 및 역할: 입력층과 출력층 사이에 존재하는 층으로, 신경망의 핵심적인 학습이 이루어지는 곳입니다.
특징: 입력 데이터를 단순 변환하는 것을 넘어, 가중합과 비선형 활성화 함수를 사용하여 데이터 속에 숨겨진 복잡한 비선형 관계나 추상적인 특징을 파악하고 모델링합니다.
<br>
2. 순전파 (Forward Propagation)
과정: 신경망의 입력층에서 시작하여 은닉층을 거쳐 출력층까지 데이터를 순차적으로 전달하며 예측값을 얻는 과정입니다.
이전 층의 노드 값에 **가중치를 곱하고 편향을 더하는 선형 변환(Z)**을 수행합니다.
이 Z 값에 **활성화 함수를 적용(A)**하여 해당 층의 출력을 계산합니다.
이 과정을 각 층마다 반복하여 최종 출력층에서 모델의 예측값을 얻습니다.
목표: 주어진 입력에 대한 모델의 예측을 생성합니다.
<br>
3. 역전파 (Backpropagation)
과정: 순전파를 통해 얻은 예측값과 실제 정답 사이의 오차(손실)를 기반으로, 신경망의 가중치와 편향을 업데이트하여 모델의 성능을 개선하는 알고리즘입니다.
출력층에서 계산된 오차를 손실 함수로 정량화합니다.
이 오차를 줄이기 위해 **연쇄 법칙(Chain Rule)**을 사용하여 네트워크의 역방향(출력층 → 은닉층 → 입력층)으로 **각 가중치와 편향에 대한 미분값(기울기)**을 효율적으로 계산합니다.
계산된 기울기 값을 활용하여 가중치와 편향을 조정합니다.
목표: 모델의 오차를 최소화하도록 파라미터를 최적화합니다.

---

## 활성화 함수

---

출력층의 활성화 함수는 신경망의 마지막 층에 위치하여, 처리된 결과를 우리가 원하는 최종 출력 형태로 변환하는 역할을 합니다.

- 회귀 문제: 
연속적인 실수 값을 예측하므로, 주로 **선형 함수(Linear Function)**를 사용합니다. (가중합 값을 그대로 사용)
- 이진 분류 문제: 
두 가지 중 하나를 분류하므로, **시그모이드 함수(Sigmoid Function)**를 사용하여 0과 1 사이의 확률 값을 출력합니다.
- 다중 분류 문제: 
여러 클래스 중 하나를 분류하므로, **소프트맥스 함수(Softmax Function)**를 사용하여 각 클래스에 속할 확률을 출력합니다.

----

## 손실 함수와 최적화

---

1. 손실 함수 (Loss Function)
역할: 모델의 예측값과 실제 정답 값 사이의 오차(차이)를 계산하는 함수입니다.
중요성: 손실 함수 값이 작을수록 모델의 예측이 정답에 더 가깝다는 의미이며, 이 값을 기준으로 모델을 학습시킵니다.
<br>
2. 경사 하강법 (Gradient Descent)
역할: 손실 함수 값을 점차 최소화하여 모델의 최적 가중치를 찾아가는 방법입니다. 손실 함수의 기울기가 가장 가파른 방향(오차가 증가하는 방향)의 반대 방향으로 조금씩 이동하며 파라미터를 업데이트합니다.
<br>
경사 하강법의 종류:
- 배치 경사 하강법 (Batch Gradient Descent)
특징: 전체 데이터셋을 한 번에 사용하여 가중치를 업데이트합니다.
장점: 손실 함수가 안정적으로 감소하며, 최적점에 수렴할 가능성이 높습니다.
단점: 대규모 데이터셋에서는 계산량이 매우 많고 느립니다.
<br>
- 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
특징: 데이터를 한 번에 한 개씩만 사용하여 가중치를 업데이트합니다.
장점: 연산 속도가 빠르고, 메모리 효율이 좋습니다. 지역 최적점에서 탈출하기 용이합니다.
단점: 가중치 업데이트 방향이 불안정하여 수렴 과정이 진동하고 느릴 수 있습니다.
<br>
- 미니배치 경사 하강법 (Mini-batch Gradient Descent)
특징: 전체 데이터를 **작은 묶음(미니배치)**으로 나누어 기울기를 계산하고 가중치를 업데이트합니다.
장점: 배치 GD와 SGD의 장점을 결합하여 연산 효율과 안정성을 동시에 얻을 수 있습니다. 가장 일반적으로 사용됩니다.
단점: 미니배치 크기 선택에 따라 성능 차이가 발생할 수 있습니다.