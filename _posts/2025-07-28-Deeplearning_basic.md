---
layout: post
title:  "Deeplearning"
date:   2025-07-28 14:00:00 +0900
categories: [딥러닝, 인공지능, 데이터과학]
tags: [딥러닝 기초]
---

------
## 머신러닝 vs 딥러닝
---

머신러닝은 데이터를 기반으로 학습하여 예측 또는 결정을 내리는 인공지능 분야입니다. 모델의 설계, 목표정의, 학습 데이터를 기반으로 하여, 최적의 Weight를 찾는 것이 머신러닝의 핵심입니다.
반면, 딥러닝은 머신러닝의 하나의 방법론이지만 인공신경망, 즉 다중 퍼셉트론(MLP)에 기반을 둡니다. 딥러닝의 경우 머신러닝과는 다르게 사람의 개입없이 데이터의 특징을 추출하고 학습합니다. 뿐만 아니라, 여러 층의 신경망을 이용하여 복잡한 패턴을 처리합니다.
이렇게 특징을 추출하는 능력을 가지고 있어, 머신러닝이 처리하기 힘든 비정형 데이터(이미지, 음성, 텍스트 등)과 같은 비정형 데이터 처리를 보다 쉽게 처리할 수 있습니다.

퍼셉트론 


다층 퍼셉트론의 등장 
- 하나의 퍼셉트론을 이용해서는 간단한 XOR문제도 해결할 수 없었다 이를 해결하기 위해 다층 퍼셉트론이 등장함

다층 퍼셉트론의 다중분류
- Softmax : i번째 클래스가 정답일 확률 pi  pi = e^z_i / sigma (e_)
- CrossEntropy

<br>

----
## 딥러닝의 성능 향상을 위해 고려해야 할 하이퍼 파라미터
---

## 1. 활성화함수(Activation Function)
인공신경망에서 뉴런의 입력의 가중치 합을 받아 출력 신호로 변환하는 함수
<br>

### 활성화 함수의 역할
- **비선형성** 도입: 각 layer는 보통 선형함수로 되어있어, 비선형 함수 없이 계속 쌓는다고 하더라도 결국 선형 모델이 되어 비선형 관계를 학습할 수 없습니다. 이러한 한계점을 극복하기 위해 비선형성을 추가해주는 역할을 합니다.
<br>

### 활성화 함수의 종류

- step function
가중합이 0보다 작으면 0, 0보다 크면 1로 반환 - 미분이 안되어서 Sigmoid를 사용하게됨

- Sigmoid 함수

| Sigmoid | 설명                                                                                                                                                                               |
|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 특징      | 입력값을 0과 1 사이의 값으로 변환                                                                                                                                                             |
| 활용      | 주로 이진 분류 문제의 출력층에서 0과 1 사이의 확률값 제공. 순환 신경망(RNN)에서 사용.                                                                                                                            | 
| 한계     | 기울기 소실(Vanishing Gradient) :입력값이 너무 크거나 작으면 기울기가 거의 0이 되어 역전파 과정에서 학습이 잘 이루어지지 않는 문제가 발생 | 

<br>

- 하이퍼볼릭 탄젠트(Tanh) 함수

| Tanh | 설명                                                   |
|------|------------------------------------------------------|
| 특징   | 시그모이드 함수를 변형한 형태로, 입력값을 -1과 1 사이의 값으로 변환       |
| 활용   | 시그모이드와 마찬가지로 순환 신경망(RNN)에서 사용                 |
| 장점   | 시그모이드 함수보다 출력값이 0을 중심으로 대칭적이기 때문에 학습이 더 효율적으로 진행 |

<br>

- ReLU (Rectified Linear Unit) 함수

| ReLU | 설명                                                               |
|-----|------------------------------------------------------------------|
| 특징  | 입력값이 0보다 작으면 0을 출력, 0보다 크면 입력값을 그대로 출력(max(0, x))                |
| 활용  | 가장 널리 사용되는 활성화 함수 중 하나                                           | 
| 장점  | 기울기 소실 문제를 완화, 계산이 단순하여 학습 속도가 빠름                                |
| 한계 | Dying ReLU(입력이 0 이하일 경우, 기울기가 0이 되어 한번 비활성화되면 활성화 되기 어려운 문제가 발생) |

<br>

- LeakyReLU

| LeakyReLU | 설명                                                         |
|-----------|------------------------------------------------------------|
| 특징        | 입력값이 0보다 작으면 아주 작은 양수 기울기가짐, 0보다 크면 입력값을 그대로 출력(max(0, x)) |
| 활용        | ReLU의 한계점인 Dying ReLU문제를 해결                                |
| 장점        | 음수 영역에서도 기울기가 존재하여 뉴런이 죽는 것을 방지, ReLU처럼 계산이 간단             |
| 한계       | 고정된 기울기가 항상 최적의 성능을 보장하지 못함                                |

<br>

- GELU

| GELU | 설명                                                                                                                                 |
|------|------------------------------------------------------------------------------------------------------------------------------------|
| 특징   | 입력값에 표준정규분포의 누적 분포 함수(CDF)를 곱하는 형태, ReLU와 Dropout의 특성을 결합하여 부드러운(smooth) 비선형성을 제공                                       |
| 활용   | BERT, GPT-3와 같은 NLP(자연어 처리) 분야의 트랜스포머(Transformer) 기반 모델에서 주로 사용. 컴퓨터 비전 분야의 Vision Transformer(ViT)나 MLP-Mixer에서도 활용됩니다 . |
| 장점   | ReLU보다 더 부드러운 비선형성을 제공, 잡음(noise)에 강인한 특성.                                                             |
| 한계  | ReLU나 LeakyReLU에 비해 계산 복잡도가 높음, 해석이 어려움                                                                                            |

<br>

## 2. 학습률(learning rate)
학습률은 옵티마이저가 손실 함수의 최소값을 찾아가는 과정에서 각 반복(iteration)마다 가중치를 업데이트하는 스텝의 크기를 결정하는 매개변수입니다 .
아래 그림과 같이, 학습률의 값이 너무 높을 경우 최적 지점을 지나쳐 발산할 위험이 있으며, 너무 낮을 경우 수렴 속도가 느려지거나 지역 최적점에 갇힐 수 있습니다.

![학습률](/assets/images/proper-step-size.png)
(이미지출처-공돌이의 수학정리노트)

많은 학습에서 **초기에는 큰 학습률**로 빠르게 학습하고, **후반부에는 작은 학습률**로 미세 조정을 합니다.

## 3. epoch, batch_size
- 에포크(epoch)란?
에포크는 전체 학습 데이터셋이 신경망 모델을 한 번 완전히 통과한 횟수를 의미합니다. 즉, 모든 훈련 데이터가 한 번씩 모델 학습에 사용되는 단위를 나타냅니다.   

- 배치 사이즈(batch size)란?
배치 사이즈는 한 번의 가중치 업데이트를 위해 사용되는 훈련 샘플의 개수입니다. 전체 데이터셋을 여러 개의 작은 그룹으로 나누었을 때, 한 소그룹에 포함되는 데이터 수를 의미하기도 합니다. 

- 에포크와 배치 사이즈가 학습에 미치는 영향
에포크 수가 너무 많으면 학습이 매우 오래 걸릴 뿐아니라, 과적합이 발생할 수 있습니다.
배치 사이즈는 일반적으로 크기가 클수록 한 에포크당 학습 속도가 빠르지만, 메모리 사용량이 증가하게 됩니다.

## 4. optimizer
옵티마이저는 모델의 손실 함수 값을 최소화하기 위해 가중치와 편향 등 모델의 파라미터들을 어떻게 업데이트할지 결정하는 알고리즘입니다. 경사 하강법 기반의 다양한 변형(예: Adam, SGD, RMSprop)이 존재하며, 최적화 과정의 효율성과 안정성에 직접적인 영향을 미칩니다.  

## 5. hidden layer 수
히든 레이어 수는 신경망의 입력층과 출력층 사이에 존재하는 은닉층의 개수를 의미합니다. 히든 레이어가 많아질수록 모델의 깊이가 깊어지며, 더 복잡하고 추상적인 데이터 특징을 학습할 수 있는 능력이 증가합니다. 그러나 과도한 레이어 수는 학습 비용을 증가시키고 과적합의 위험을 높일 수 있습니다.

## 6. dropout
드롭아웃은 신경망의 과적합(Overfitting)을 방지하기 위한 정규화(Regularization) 기법 중 하나입니다. 학습 과정에서 무작위로 선택된 뉴런들을 임시적으로 비활성화시켜 업데이트에 참여시키지 않음으로써, 각 뉴런이 다른 뉴런에 과도하게 의존하는 것을 방지하고 모델의 일반화 성능을 향상시키는 효과를 가져옵니다.

![드롭아웃](/assets/images/dropout.png)
(이미지출처-a simple way to prevent neural networks from overfitting)
