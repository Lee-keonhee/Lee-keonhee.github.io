---
layout: post
title:  "CNN(Convolutional Neural Network"
date:   2025-07-29 16:00:00 +0900
categories: [딥러닝, 인공지능, 데이터과학]
tags: [딥러닝 기초]
---

------

## 컴퓨터는 이미지를 어떻게 볼까?

---

흑백이미지의 경우, 이미지의 각 pixel의 명암을 기준으로 점점 밝아지면 255, 어두워지면 0에 가깝게 인식하여 이미지를 인식한다.
컬러 이미지의 경우, 각 이미지가 R, G, B 3개의 채널로 이루어져있으며, 각 이미지의 색상의 강도를 0~255 값으로 인식한다.

---

## FCN의 한계는?

---

이미지일 경우, FCN에 입력하기 위해서는 각 이미지를 Flatten(평탄화) 작업을 하여, 각 픽셀 간의 위치정보를 잊게된다. 따라서, 인접 픽셀간의 관계를 학습할 수 없어 패턴학습이 어려워진다.
뿐만 아니라, 모든 픽셀을 연결해 하나의 output을 도출해야하므로 파라미터의 수가 매우 늘어나게 되고 과적합 및 계산 비용증가의 문제점이 발생할 수 있다.


---

## CNN

---

이러한 FCN의 문제점을 극복하기 위해, Kernel을 이용한 합성곱(Convolution) 연산을 통해 지역적인 특징을 추출할 수 있는 신경망으로써, 주로 이미지 인식 및 분류에 특화된 인공신경망의 한 종류

#### CNN의 특징
- 부분 연결:
모든 노드 연결방식을 대체, 전체 이미지를 한번에 분석하지않고, 근접픽셀간의 관계부터 점진적 학습, 이미지의 지역적 정보를 효과적으로 학습
- 가중치 공유
동일한 필터(커널)을 이미지 전체에 적용, 파라미터의 수를 크게 줄여 계산 효율성을 향상
- 계층적 특징 표현
입력에 가까운 hidden layer에서는 단순한 특징(에지, 선)을 학습
출력에 가까운 hidden layer에는 추상적이고 복잡한 특징을 학습

#### Convolutional Filter
- 커널(Kernel)이라고도 부르며, 이미지의 크기보다 작은 행렬
- 해당 Filter의 크기만큼의 영역에 대해 계산 수행
- 이미지 패턴 및 특징을 인식
- 색상이 있는 이미지는 RGB 채널에 각각의 가중치를 가지는 필터를 적용

#### Feature Map
Filter를 통한 합성곱 연산 후에 나오는 출력값들의 2차원 배열로써, 시각적 특징들을 저장한 배열
입력 데이터에서 한 가지 특정한 특징 정보를 추출하고 그 위치와 강도를 2차원 형태로 표현한 결과물
-> 즉, 여러가지 필터를 통해, 입력 이미지의 서로 다른 종류의 특징을 감지 
(ex. 저수준 : 선, 모서리, 색상 변화 등, 고수준: 완전한 객체(고양이, 자동차), 전체적인 장면 등)

![kernel 계산](/assets/images/CNN_kernel1.png)
![kernel 계산](/assets/images/CNN_kernel2.png)
![kernel 계산](/assets/images/CNN_kernel3.png)

1. 커널과 입력값 영역 선택
입력 이미지에서 커널 크기만큼 영역 선택
2. 곱셈 및 덧셈 연산
선택한 영역과 커널 원소별 곱셈 후 편향값과 함께 더함
3. 특성 맵 생성
출력값을 특성 맵에 저장 및 커널을 입력 이미지 전체에 스트라이드만큼 이동하며 반복

Convolution 연산은 그 자체만으론 선형 계산이기 때문에, Activation funciton을 통해 비선형성 추가 -> 복잡한 데이터 학습

---

## Padding, Stride

----

#### 패딩(Padding)

이미지 외곽에 픽셀 값을 추가(주로 0,zero padding)하여, 출력이미지의 크기를 조절함.
목적
- 필터 연산 시 이미지 가장자리 정보 손실 방지
- 출력 크기를 조정하여 원하는 네트워크 구조 유지

#### Stride

필터의 이동 간격, 필터가 한번 이동할때 움직이는 pixel의 간격.
스트라이드가 커지게 되면 출력 특징맵이 작아지게됨.
합성곱 연산의 복잡도 줄임.

출력 feature맵 크기 계산
Output_Size = int((Input Size - Filter Size + 2 * Padding) / Stride) + 1

---

## Receptive Field

---
CNN에서 출력(결과) 이미지의 한 점(픽셀/뉴런)이 입력(원본) 이미지의 어느 영역에서 영향을 받아 만들어졌는지를 나타내는 범위

[//]: # (리셉티브 필드&#40;RF&#41;는 네트워크의 출력 층에서 시작하여 입력 층으로 역추적하면서 계산합니다. 각 층의 필터 크기&#40;커널 사이즈&#41;, 스트라이드&#40;Stride&#41;, 패딩&#40;Padding&#41; 값이 영향을 미치는데, 특히 필터 크기와 스트라이드가 핵심입니다.)

[//]: # ()
[//]: # (간단한 계산 공식은 다음과 같습니다 &#40;패딩은 0이라고 가정&#41;:)

[//]: # ()
[//]: # (각 층의 효과 계산: 각 층이 RF에 기여하는 '크기'와 '점프' 값을 계산합니다.)

[//]: # ()
[//]: # (r_L: 해당 층의 출력 뉴런이 입력 이미지에서 '보는' 크기 &#40;리셉티브 필드 크기&#41;)

[//]: # (s_L: 해당 층의 스트라이드)

[//]: # (역방향 계산: 출력 층에서 입력 층으로 거슬러 올라갑니다.)

[//]: # ()
[//]: # (초기값: 최종 출력 층의 한 뉴런의 RF는 1입니다. &#40;rf_output = 1&#41;)

[//]: # ()
[//]: # (각 층을 역추적하며 RF 계산: 이전 층의 RF 크기 = &#40;현재 층의 RF 크기 - 1&#41; * 현재 층의 스트라이드 + 현재 층의 커널 크기)

[//]: # ()
[//]: # (예시)

[//]: # (자, 2개의 합성곱 층으로 구성된 간단한 신경망을 예로 들어볼게요.)

[//]: # ()
[//]: # (Layer 1 &#40;Conv1&#41;:)

[//]: # (커널 크기 &#40;k1&#41; = 3)

[//]: # (스트라이드 &#40;s1&#41; = 1)

[//]: # (Layer 2 &#40;Conv2&#41;:)

[//]: # (커널 크기 &#40;k2&#41; = 3)

[//]: # (스트라이드 &#40;s2&#41; = 2)

[//]: # (계산 과정)

[//]: # (최종 출력 &#40;Conv2 출력&#41;의 RF:)

[//]: # ()
[//]: # (가장 마지막 층인 Conv2의 한 픽셀이 Conv2의 입력&#40;Conv1의 출력&#41;에서 "보는" 크기는 Conv2의 커널 크기인 3입니다.)

[//]: # (이 시점의 RF는 r = 1 로 시작합니다. &#40;출력 픽셀 자체&#41;)

[//]: # (Conv2 → Conv1로 역추적:)

[//]: # ()
[//]: # (Conv2의 한 픽셀은 Conv1의 출력에서 k2=3만큼의 영역을 봅니다.)

[//]: # (이 3개의 픽셀이 Conv1의 스트라이드 s1=1만큼 원본 입력에서 이동하므로,)

[//]: # (Conv1의 출력 픽셀 3개가 원본 입력에서 보는 영역:)

[//]: # (중앙 픽셀의 RF: 3 &#40;Conv2의 커널 크기&#41;)

[//]: # (j_prev_layer = 1 &#40;이전 층의 스트라이드&#41;)

[//]: # (공식: rf_prev = rf_current + &#40;k_current - 1&#41; * s_accumulated_before_current_layer &#40;조금 복잡하죠&#41;)

[//]: # (더 쉬운 방법: 각 층의 출력에서 RF가 입력에 대해 얼마나 커지는지를 더해가는 방식)

[//]: # ()
[//]: # (1단계 &#40;Conv1의 출력 픽셀이 원본 입력에서 보는 RF&#41;:)

[//]: # ()
[//]: # (k1 = 3, s1 = 1)

[//]: # (Conv1의 한 픽셀은 원본 입력에서 k1 = 3만큼의 영역을 봅니다.)

[//]: # (RF_Conv1 = 3)

[//]: # (2단계 &#40;Conv2의 출력 픽셀이 원본 입력에서 보는 RF&#41;:)

[//]: # ()
[//]: # (Conv2의 한 픽셀은 Conv1의 출력에서 k2 = 3만큼의 영역을 봅니다.)

[//]: # (이 k2만큼의 영역이 다시 원본 입력에 매핑될 때, Conv1의 스트라이드 s1만큼의 '점프'를 고려해야 합니다.)

[//]: # (RF_Conv2 = RF_Conv1 + &#40;k2 - 1&#41; * &#40;Conv1까지의 누적 스트라이드&#41;)

[//]: # (RF_Conv2 = 3 + &#40;3 - 1&#41; * 1 &#40;Conv1까지의 누적 스트라이드는 Conv1 자체의 스트라이드 s1=1&#41;)

[//]: # (RF_Conv2 = 3 + 2 * 1 = 5)

[//]: # (따라서, Conv2의 출력에서 한 픽셀은 원본 입력 이미지의 5x5 영역을 보게 됩니다.)


----
## Pooling Layer
----
대충 다운샘플링, 차원 축소 이런거하는 층 - 계산량 감소
Max Pooling, Average Pooling
풀링의 크기와 stride 크기 같게해서 많이 사용
풀링커널은 가중치없고, 해당 데이터의 max, mean 채용


장점 - 계산량 감소, 과적합 방지, 위치 불변성 강화(작은 변형에 특징 유지함)
단점 - 정보 손실가능성, 위치정보 손실, 고정된 연산방식

---
## CNN 모델
---

convolutional Block = Convolutional Layer(convolution oper + activaiton) + Pooling

CNN 모델의 순서 : input - Convolutional Block * n - Flatten layer - FCL_classification - softmax - output


---

## LeNet-5

---

손글자 숫자(MNIST 데이터 셋) 인식 및 분류를 위해 CNN의 최초 실용적 구현한 모델.
합성곱 층(CNN Layer)와 풀링 층(Pooling Layer)라는 구성요소 등장
